{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d7d9c6-d1bf-42fe-b86c-32f6dcd557bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import torch\n",
    "from sindy.SINDy_library import *\n",
    "from autoencoder.autoencoder import AutoEncoder\n",
    "import math\n",
    "\n",
    "\n",
    "class SINDy(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    SINDy dz predictions\n",
    "\n",
    "    Arguments:\n",
    "        params - Dictionary object containing the parameters that specify the training.\n",
    "        See params.txt file for a description of the parameters.\n",
    "\n",
    "    Returns:\n",
    "        sindy_predict - tensor containing sindy's predictions for dz.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, encoder:AutoEncoder, decoder:AutoEncoder, params:Dict = {}, *args, **kwargs) -> None:\n",
    "      super().__init__(*args, **kwargs) \n",
    "      self.params = params\n",
    "      self.encoder = encoder\n",
    "      self.decoder =decoder\n",
    "\n",
    "      self.input_dim = self.params['input_dim']\n",
    "      self.latent_dim = self.params['latent_dim']\n",
    "      self.poly_order = self.params['poly_order']\n",
    "      if 'include_sine' in self.params.keys():\n",
    "        self.include_sine = self.params['include_sine']\n",
    "      else:\n",
    "        self.include_sine = False\n",
    "      self.library_dim = self.params['library_dim']\n",
    "      self.model_order = self.params['model_order']\n",
    "      self.sequential_thresholding = self.params['sequential_thresholding']\n",
    "      self.coefficient_initialization = self.params['coefficient_initialization']\n",
    "      self.coefficient_mask = self.params['coefficient_initialization']\n",
    "      self.coefficient_threshold = self.params['coefficient_threshold']\n",
    "      #initialize sindy coefficients  \n",
    "      self.sindy_coefficients = torch.zeros((library_size(self.latent_dim, self.poly_order), self.latent_dim))\n",
    "      self.init_sindy_coefficients()\n",
    "\n",
    "\n",
    "    def init_sindy_coefficients(self, name='normal', std=1., k=3):\n",
    "\n",
    "      #self.sindy_coefficients = std*torch.randn_like(self.sindy_coefficients)\n",
    " \n",
    "      if name == 'xavier':\n",
    "        self.sindy_coefficients = torch.nn.init.xavier_uniform_(self.sindy_coefficients)\n",
    "      elif name == 'uniform':\n",
    "        self.sindy_coefficients = torch.nn.init.uniform_(self.sindy_coefficients, low=0.0, high=1.0)\n",
    "      elif name == 'constant':\n",
    "        self.sindy_coefficients = torch.ones_like(self.sindy_coefficients)*k\n",
    "      elif name == 'normal':\n",
    "        self.sindy_coefficients = torch.nn.init.normal_(self.sindy_coefficients, mean=0, std=std) \n",
    "    \n",
    "      \n",
    "\n",
    "    def forward(self, x, dx, ddx)-> torch.Tensor:\n",
    "\n",
    "      #x = x.reshape(1, *x.shape)\n",
    "      #dx = dx.reshape(1, *dx.shape)\n",
    "      \n",
    "      z = self.encoder(torch.cat((x, dx)))\n",
    "      dz = z[z.shape[0]//2:]\n",
    "      z = z[:z.shape[0]//2]\n",
    "      \n",
    "      #create Theta\n",
    "      if self.model_order == 1:\n",
    "        Theta = sindy_library_pt(z, self.latent_dim, self.poly_order, self.include_sine)\n",
    "      else:\n",
    "        Theta = sindy_library_pt_order2(z, dz, self.latent_dim, self.poly_order, self.include_sine)\n",
    "      print(Theta.shape)\n",
    "      #apply thresholding or not\n",
    "      if self.sequential_thresholding:\n",
    "        '''\n",
    "        tmp = torch.rand(size=(library_dim,latent_dim), dtype=torch.float32)\n",
    "        mask = torch.zeros_like(tmp)\n",
    "        mask = mask.where(self.coefficient_mask, tmp)\n",
    "        '''\n",
    "        mask = torch.where(self.sindy_coefficients > self.coefficient_threshold, self.sindy_coefficients, 0)\n",
    "        sindy_predict = torch.matmul(Theta, mask*self.sindy_coefficients)\n",
    "      else:\n",
    "        sindy_predict = torch.matmul(Theta, self.sindy_coefficients)\n",
    "\n",
    "      #decode\n",
    "      x_decode = self.decoder(torch.cat((z, dz)))\n",
    "      dx_decode = x_decode[x_decode.shape[0]//2:]\n",
    "      x_decode = x_decode[:x_decode.shape[0]//2]\n",
    "\n",
    "      if self.model_order == 1:\n",
    "       dz_predict = sindy_predict\n",
    "       dzz_predict = None\n",
    "      else:\n",
    "       ddz_predict = sindy_predict\n",
    "       dz_predict = None\n",
    "\n",
    "      return torch.cat((x, dx)), torch.cat((dz_predict, dz)), torch.cat((x_decode, dx_decode)), self.sindy_coefficients, sindy_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f06b777c-3b7e-42f0-bdb7-3ad7103b353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.]]),\n",
       " tensor([[ 3.2705e+00,  1.3477e+00,  4.6347e+00],\n",
       "         [ 3.2705e+00,  1.3477e+00,  4.6347e+00],\n",
       "         [ 1.0530e-03,  3.3623e-03, -2.0413e-03],\n",
       "         [ 1.0530e-03,  3.3623e-03, -2.0413e-03]], grad_fn=<CatBackward0>),\n",
       " tensor([[ 4.3785e-01,  4.3267e-01,  5.0612e-01,  4.4630e-01,  6.5727e-01,\n",
       "           5.0583e-01,  4.8784e-01,  4.1336e-01,  4.4768e-01,  5.2676e-01,\n",
       "           5.1867e-01,  5.5566e-01,  5.2292e-01,  5.3274e-01,  5.3530e-01,\n",
       "           5.3238e-01,  6.1830e-01,  4.7802e-01,  5.3967e-01,  5.6487e-01,\n",
       "           5.7269e-01,  5.4722e-01,  5.9539e-01,  5.7667e-01,  4.0526e-01,\n",
       "           5.9492e-01,  4.4905e-01,  4.8572e-01,  4.7597e-01,  5.6002e-01,\n",
       "           4.8754e-01,  6.3812e-01,  5.5217e-01,  5.3112e-01,  5.2920e-01,\n",
       "           5.7368e-01,  4.1302e-01,  4.8592e-01,  4.8918e-01,  3.8134e-01,\n",
       "           4.7153e-01,  5.9888e-01,  3.9877e-01,  5.4279e-01,  6.2383e-01,\n",
       "           5.0650e-01,  5.6862e-01,  4.1519e-01,  6.9801e-01,  5.5883e-01,\n",
       "           5.8222e-01,  6.0691e-01,  4.9760e-01,  6.2501e-01,  5.9283e-01,\n",
       "           4.6888e-01,  4.7314e-01,  5.9360e-01,  5.2298e-01,  4.2888e-01,\n",
       "           4.3485e-01,  4.4685e-01,  4.4240e-01,  4.9703e-01,  4.9996e-01,\n",
       "           5.5179e-01,  6.6201e-01,  3.8733e-01,  4.8126e-01,  4.8806e-01,\n",
       "           5.7097e-01,  4.8589e-01,  3.6005e-01,  5.1559e-01,  5.9945e-01,\n",
       "           5.5214e-01,  4.7585e-01,  6.1018e-01,  5.3780e-01,  3.6070e-01,\n",
       "           5.1783e-01,  4.5074e-01,  5.3459e-01,  3.6404e-01,  4.2148e-01,\n",
       "           5.1741e-01,  5.3499e-01,  3.2046e-01,  5.7240e-01,  4.8918e-01,\n",
       "           5.6312e-01,  4.5756e-01,  6.1937e-01,  4.8794e-01,  5.1304e-01,\n",
       "           4.5083e-01,  5.9661e-01,  5.4886e-01,  4.3147e-01,  4.4035e-01,\n",
       "           5.9902e-01,  4.7029e-01,  5.4444e-01,  5.4741e-01,  4.2390e-01,\n",
       "           4.7323e-01,  3.8358e-01,  5.4251e-01,  5.4618e-01,  4.8963e-01,\n",
       "           4.4039e-01,  6.7292e-01,  5.7684e-01,  4.8376e-01,  6.1929e-01,\n",
       "           6.1430e-01,  5.4564e-01,  4.6093e-01,  4.9713e-01,  4.1883e-01,\n",
       "           5.9015e-01,  4.4559e-01,  6.3560e-01,  5.6673e-01,  5.2829e-01,\n",
       "           5.5515e-01,  5.6897e-01,  4.9236e-01],\n",
       "         [ 4.3785e-01,  4.3267e-01,  5.0612e-01,  4.4630e-01,  6.5727e-01,\n",
       "           5.0583e-01,  4.8784e-01,  4.1336e-01,  4.4768e-01,  5.2676e-01,\n",
       "           5.1867e-01,  5.5566e-01,  5.2292e-01,  5.3274e-01,  5.3530e-01,\n",
       "           5.3238e-01,  6.1830e-01,  4.7802e-01,  5.3967e-01,  5.6487e-01,\n",
       "           5.7269e-01,  5.4722e-01,  5.9539e-01,  5.7667e-01,  4.0526e-01,\n",
       "           5.9492e-01,  4.4905e-01,  4.8572e-01,  4.7597e-01,  5.6002e-01,\n",
       "           4.8754e-01,  6.3812e-01,  5.5217e-01,  5.3112e-01,  5.2920e-01,\n",
       "           5.7368e-01,  4.1302e-01,  4.8592e-01,  4.8918e-01,  3.8134e-01,\n",
       "           4.7153e-01,  5.9888e-01,  3.9877e-01,  5.4279e-01,  6.2383e-01,\n",
       "           5.0650e-01,  5.6862e-01,  4.1519e-01,  6.9801e-01,  5.5883e-01,\n",
       "           5.8222e-01,  6.0691e-01,  4.9760e-01,  6.2501e-01,  5.9283e-01,\n",
       "           4.6888e-01,  4.7314e-01,  5.9360e-01,  5.2298e-01,  4.2888e-01,\n",
       "           4.3485e-01,  4.4685e-01,  4.4240e-01,  4.9703e-01,  4.9996e-01,\n",
       "           5.5179e-01,  6.6201e-01,  3.8733e-01,  4.8126e-01,  4.8806e-01,\n",
       "           5.7097e-01,  4.8589e-01,  3.6005e-01,  5.1559e-01,  5.9945e-01,\n",
       "           5.5214e-01,  4.7585e-01,  6.1018e-01,  5.3780e-01,  3.6070e-01,\n",
       "           5.1783e-01,  4.5074e-01,  5.3459e-01,  3.6404e-01,  4.2148e-01,\n",
       "           5.1741e-01,  5.3499e-01,  3.2046e-01,  5.7240e-01,  4.8918e-01,\n",
       "           5.6312e-01,  4.5756e-01,  6.1937e-01,  4.8794e-01,  5.1304e-01,\n",
       "           4.5083e-01,  5.9661e-01,  5.4886e-01,  4.3147e-01,  4.4035e-01,\n",
       "           5.9902e-01,  4.7029e-01,  5.4444e-01,  5.4741e-01,  4.2390e-01,\n",
       "           4.7323e-01,  3.8358e-01,  5.4251e-01,  5.4618e-01,  4.8963e-01,\n",
       "           4.4039e-01,  6.7292e-01,  5.7684e-01,  4.8376e-01,  6.1929e-01,\n",
       "           6.1430e-01,  5.4564e-01,  4.6093e-01,  4.9713e-01,  4.1883e-01,\n",
       "           5.9015e-01,  4.4559e-01,  6.3560e-01,  5.6673e-01,  5.2829e-01,\n",
       "           5.5515e-01,  5.6897e-01,  4.9236e-01],\n",
       "         [-5.3371e-06, -3.5294e-06,  1.2179e-05,  2.3368e-07, -4.2604e-06,\n",
       "           5.0349e-06,  2.8291e-06, -3.8951e-06,  1.5168e-06,  1.4808e-05,\n",
       "           1.8000e-07,  4.5463e-06, -6.8834e-06,  5.7432e-06, -4.0634e-06,\n",
       "          -1.1566e-05, -2.0249e-06, -2.3240e-06,  3.2500e-06, -3.4619e-06,\n",
       "           7.0040e-07, -4.2914e-06, -6.8250e-06,  3.6755e-06, -4.6208e-06,\n",
       "           5.2448e-07, -6.2723e-07, -2.3489e-06, -1.0968e-06,  1.1984e-05,\n",
       "          -5.3875e-06, -5.3467e-06,  1.0273e-05, -6.4455e-06,  1.0064e-06,\n",
       "          -6.0148e-06, -8.7783e-06, -1.4317e-05, -3.8263e-06, -1.4517e-05,\n",
       "          -3.5557e-06,  8.9749e-06,  3.6959e-07,  1.7473e-07, -8.5024e-06,\n",
       "          -8.5625e-07,  3.1465e-07,  2.6660e-06, -2.3233e-06,  2.8479e-06,\n",
       "          -9.1196e-07, -5.7676e-06,  7.9051e-07,  2.0265e-06,  1.0566e-05,\n",
       "          -1.6463e-06,  7.0426e-06,  5.4359e-06, -6.3401e-06, -1.0563e-06,\n",
       "           5.7305e-06, -5.4336e-06,  6.6241e-06,  4.7501e-06,  3.7191e-06,\n",
       "           5.9290e-06, -3.4057e-06,  4.2083e-06, -6.9652e-06, -7.7237e-06,\n",
       "           5.6677e-06,  4.5515e-06,  2.2098e-07,  1.6048e-05,  2.7753e-06,\n",
       "          -9.9299e-07,  9.2666e-06,  4.9376e-06,  3.1709e-06, -1.3487e-06,\n",
       "          -5.1608e-07, -2.0093e-06,  8.7110e-07,  1.0632e-05,  3.1248e-06,\n",
       "          -7.5117e-06,  1.1164e-06,  9.8813e-06,  4.7385e-06,  8.1223e-06,\n",
       "           4.5207e-06, -6.2917e-06,  5.2348e-06,  3.8427e-06, -3.6678e-06,\n",
       "           5.4640e-06,  3.3329e-06,  7.5010e-06,  3.9903e-06, -2.0412e-06,\n",
       "          -4.2071e-06, -3.4174e-06,  2.5963e-06,  3.1462e-06,  6.3826e-06,\n",
       "          -1.1030e-06,  3.7235e-06, -1.0394e-05,  6.0953e-06, -9.6565e-07,\n",
       "          -9.0843e-06,  1.7738e-06,  5.0050e-06, -5.0331e-06, -6.8866e-06,\n",
       "           4.3869e-06,  2.1227e-06,  5.7276e-06,  2.5210e-06, -1.4438e-05,\n",
       "           1.1855e-06, -1.0758e-05, -5.3015e-06,  5.5966e-06,  1.4144e-06,\n",
       "          -5.2679e-06,  2.2461e-06,  1.1061e-05],\n",
       "         [-5.3371e-06, -3.5294e-06,  1.2179e-05,  2.3368e-07, -4.2604e-06,\n",
       "           5.0349e-06,  2.8291e-06, -3.8951e-06,  1.5168e-06,  1.4808e-05,\n",
       "           1.8000e-07,  4.5463e-06, -6.8834e-06,  5.7432e-06, -4.0634e-06,\n",
       "          -1.1566e-05, -2.0249e-06, -2.3240e-06,  3.2500e-06, -3.4619e-06,\n",
       "           7.0040e-07, -4.2914e-06, -6.8250e-06,  3.6755e-06, -4.6208e-06,\n",
       "           5.2448e-07, -6.2723e-07, -2.3489e-06, -1.0968e-06,  1.1984e-05,\n",
       "          -5.3875e-06, -5.3467e-06,  1.0273e-05, -6.4455e-06,  1.0064e-06,\n",
       "          -6.0148e-06, -8.7783e-06, -1.4317e-05, -3.8263e-06, -1.4517e-05,\n",
       "          -3.5557e-06,  8.9749e-06,  3.6959e-07,  1.7473e-07, -8.5024e-06,\n",
       "          -8.5624e-07,  3.1465e-07,  2.6660e-06, -2.3233e-06,  2.8479e-06,\n",
       "          -9.1196e-07, -5.7676e-06,  7.9051e-07,  2.0265e-06,  1.0566e-05,\n",
       "          -1.6463e-06,  7.0426e-06,  5.4359e-06, -6.3401e-06, -1.0563e-06,\n",
       "           5.7305e-06, -5.4336e-06,  6.6241e-06,  4.7501e-06,  3.7191e-06,\n",
       "           5.9290e-06, -3.4057e-06,  4.2083e-06, -6.9652e-06, -7.7237e-06,\n",
       "           5.6677e-06,  4.5515e-06,  2.2098e-07,  1.6048e-05,  2.7753e-06,\n",
       "          -9.9299e-07,  9.2666e-06,  4.9376e-06,  3.1709e-06, -1.3487e-06,\n",
       "          -5.1608e-07, -2.0093e-06,  8.7110e-07,  1.0632e-05,  3.1248e-06,\n",
       "          -7.5117e-06,  1.1164e-06,  9.8813e-06,  4.7385e-06,  8.1223e-06,\n",
       "           4.5207e-06, -6.2917e-06,  5.2348e-06,  3.8427e-06, -3.6678e-06,\n",
       "           5.4640e-06,  3.3329e-06,  7.5010e-06,  3.9903e-06, -2.0412e-06,\n",
       "          -4.2071e-06, -3.4174e-06,  2.5963e-06,  3.1462e-06,  6.3826e-06,\n",
       "          -1.1030e-06,  3.7235e-06, -1.0394e-05,  6.0953e-06, -9.6565e-07,\n",
       "          -9.0843e-06,  1.7738e-06,  5.0050e-06, -5.0331e-06, -6.8866e-06,\n",
       "           4.3869e-06,  2.1227e-06,  5.7276e-06,  2.5210e-06, -1.4438e-05,\n",
       "           1.1855e-06, -1.0758e-05, -5.3015e-06,  5.5966e-06,  1.4144e-06,\n",
       "          -5.2679e-06,  2.2461e-06,  1.1061e-05]], grad_fn=<CatBackward0>),\n",
       " tensor([[-0.0388, -1.2218,  1.4427],\n",
       "         [ 0.5507, -0.1114, -0.3029],\n",
       "         [ 0.1980, -0.2322,  1.3094],\n",
       "         [ 1.9856, -0.1467,  0.9328],\n",
       "         [-0.4183,  0.4582, -0.3169],\n",
       "         [-0.0767,  0.7476,  0.7232],\n",
       "         [ 0.4788,  0.4828,  0.5216],\n",
       "         [ 0.1948, -0.6990, -0.3157],\n",
       "         [-1.2507, -0.8479, -1.0125],\n",
       "         [-0.9967, -0.0058,  1.4049],\n",
       "         [ 0.4900,  1.0649, -0.0485],\n",
       "         [ 0.4596, -0.3555, -1.3161],\n",
       "         [ 0.0488, -0.9320, -1.3398],\n",
       "         [-0.6584, -0.5968,  1.1028],\n",
       "         [-0.1445, -0.6769, -1.5202],\n",
       "         [ 1.0815, -1.7141,  0.2650],\n",
       "         [ 0.0618,  0.4892, -1.0333],\n",
       "         [ 1.6451, -0.3508, -0.3780],\n",
       "         [-0.8970,  1.5289, -0.6023],\n",
       "         [-0.0157,  1.4905, -0.5709]]),\n",
       " tensor([[3.2705, 1.3477, 4.6347],\n",
       "         [3.2705, 1.3477, 4.6347]], grad_fn=<MmBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "params = pickle.load(open('utils/model1_params.pkl', 'rb'))\n",
    "encoder = AutoEncoder(params, 'encoder')\n",
    "decoder = AutoEncoder(params, 'decoder')\n",
    "sindy = SINDy(encoder, decoder, params)\n",
    "\n",
    "x = torch.ones((2,128))\n",
    "dx = torch.ones((2,128))\n",
    "ddx = torch.ones((64,64))\n",
    "\n",
    "sindy(x, dx, ddx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
